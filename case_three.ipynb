{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4cf389",
   "metadata": {},
   "source": [
    "# Case 3: Adversarial Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c492c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hypertension",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "heart_disease",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bmi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hbA1c_level",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "blood_glucose_level",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "gender_Male",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "race",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "diabetes",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e6489d19-c73c-4d12-a345-e690d61ddf8a",
       "rows": [
        [
         "0",
         "75.0",
         "0",
         "1",
         "27.32",
         "5.7",
         "155",
         "1",
         "Caucasian",
         "0"
        ],
        [
         "1",
         "51.0",
         "0",
         "0",
         "32.57",
         "5.8",
         "159",
         "0",
         "Caucasian",
         "0"
        ],
        [
         "2",
         "61.0",
         "0",
         "0",
         "27.32",
         "6.5",
         "160",
         "0",
         "Caucasian",
         "0"
        ],
        [
         "3",
         "40.0",
         "0",
         "0",
         "31.82",
         "8.2",
         "126",
         "0",
         "Caucasian",
         "1"
        ],
        [
         "4",
         "53.0",
         "0",
         "0",
         "23.82",
         "6.5",
         "85",
         "0",
         "Caucasian",
         "0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>bmi</th>\n",
       "      <th>hbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>race</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.57</td>\n",
       "      <td>5.8</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.32</td>\n",
       "      <td>6.5</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.82</td>\n",
       "      <td>8.2</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.82</td>\n",
       "      <td>6.5</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  hypertension  heart_disease    bmi  hbA1c_level  blood_glucose_level  \\\n",
       "0  75.0             0              1  27.32          5.7                  155   \n",
       "1  51.0             0              0  32.57          5.8                  159   \n",
       "2  61.0             0              0  27.32          6.5                  160   \n",
       "3  40.0             0              0  31.82          8.2                  126   \n",
       "4  53.0             0              0  23.82          6.5                   85   \n",
       "\n",
       "   gender_Male       race  diabetes  \n",
       "0            1  Caucasian         0  \n",
       "1            0  Caucasian         0  \n",
       "2            0  Caucasian         0  \n",
       "3            0  Caucasian         1  \n",
       "4            0  Caucasian         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "x = pd.read_csv('data/X_caucasian_biased.csv')\n",
    "y = pd.read_csv('data/y_caucasian_biased.csv')\n",
    "df = pd.concat([x, y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79567292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S Loss Weights (0/1): tensor([0.7009, 0.2991])\n",
      "✅ Scaler successfully saved to scaler/adversarial_scaler.joblib\n",
      "Input Size: 7, Y Classes: 2, S Groups: 2\n",
      "Train samples: 15920, Test samples: 3981\n"
     ]
    }
   ],
   "source": [
    "# data_prep.py (MODIFIED FOR BINARY SENSITIVE ATTRIBUTE)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'your_data.csv' \n",
    "TARGET_COLUMN = 'diabetes'    \n",
    "SENSITIVE_COLUMN = 'race'   \n",
    "\n",
    "# Dummy Data Creation (Same as before)\n",
    "def create_dummy_data():\n",
    "    N_ROWS = 20000\n",
    "    df = pd.DataFrame({\n",
    "        f'feature_{i}': np.random.rand(N_ROWS) for i in range(5)\n",
    "    })\n",
    "    df['target'] = np.random.randint(0, 2, N_ROWS)\n",
    "    df['feature_6'] = np.random.rand(N_ROWS)\n",
    "    df[SENSITIVE_COLUMN] = np.random.choice(['Caucasian','Asian','AfricanAmerican','Hispanic','Other'], N_ROWS)\n",
    "    return df.drop(columns=['feature_6']) \n",
    "\n",
    "try:\n",
    "    data = df.copy()\n",
    "except FileNotFoundError:\n",
    "    print(f\"File {DATA_FILE} not found. Creating dummy data...\")\n",
    "    data = create_dummy_data()\n",
    "\n",
    "# --- 1. Encoding and Splitting ---\n",
    "\n",
    "# 1.1 Binarize the sensitive attribute (S)\n",
    "# Privileged: Caucasian (1), Unprivileged: All Others (0)\n",
    "# This results in a binary target for the Adversary.\n",
    "data['SENSITIVE_BINARIZED'] = data[SENSITIVE_COLUMN].apply(\n",
    "    lambda x: 1 if x == 'Caucasian' else 0\n",
    ")\n",
    "\n",
    "# Set the new sensitive groups count\n",
    "NUM_SENSITIVE_GROUPS = 2 # Now strictly binary!\n",
    "\n",
    "# Separate features (X), primary target (Y), and sensitive target (S)\n",
    "X = data.drop(columns=[TARGET_COLUMN, SENSITIVE_COLUMN, 'SENSITIVE_BINARIZED'])\n",
    "Y = data[TARGET_COLUMN]\n",
    "S = data['SENSITIVE_BINARIZED'] # Use the new binary target\n",
    "\n",
    "# 1.2 Train/Test Split (80/20) - CRUCIAL STEP\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(\n",
    "    X, Y, S, test_size=0.2, random_state=42, stratify=Y \n",
    ")\n",
    "\n",
    "# Calculate class frequencies for the sensitive attribute S\n",
    "s_counts = S_train.value_counts()\n",
    "s_weights = 1.0 / s_counts\n",
    "s_weights = s_weights / s_weights.sum() # Normalize weights\n",
    "\n",
    "# Convert to a tensor for PyTorch\n",
    "S_LOSS_WEIGHTS = torch.tensor(s_weights.sort_index().values, dtype=torch.float32)\n",
    "\n",
    "# Add S_LOSS_WEIGHTS to your print statement so you know the weights:\n",
    "print(f\"S Loss Weights (0/1): {S_LOSS_WEIGHTS}\")\n",
    "# (If 0 is the minority, its weight should be much higher than 1's weight)\n",
    "\n",
    "# --- 2. Normalization (Preventing Leakage) ---\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "filename = \"scaler/adversarial_scaler.joblib\"\n",
    "joblib.dump(scaler, filename)\n",
    "print(f\"✅ Scaler successfully saved to {filename}\")\n",
    "# --- 3. Custom PyTorch Dataset (Same structure, different S data) ---\n",
    "\n",
    "class ColumnarDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data, S_data):\n",
    "        self.X = torch.tensor(X_data, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y_data.values, dtype=torch.long)\n",
    "        self.S = torch.tensor(S_data.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx], self.S[idx]\n",
    "\n",
    "# Create the final Dataset objects\n",
    "train_dataset = ColumnarDataset(X_train_scaled, Y_train, S_train)\n",
    "test_dataset = ColumnarDataset(X_test_scaled, Y_test, S_test)\n",
    "\n",
    "# --- 4. DataLoader Setup ---\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = X_train_scaled.shape[1] \n",
    "NUM_CLASSES = len(Y.unique())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Input Size: {INPUT_SIZE}, Y Classes: {NUM_CLASSES}, S Groups: {NUM_SENSITIVE_GROUPS}\")\n",
    "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84af56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial_model.py (Adversary output MODIFIED to 2 logits, Architecture is FATTER)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Model 1: Feature Extractor (F) (No change) ---\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_size, feature_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, feature_dim), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Model 2: Primary Classifier (C) (No change) ---\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_dim, num_classes) \n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "# --- Model 3: Adversary (A) (MODIFIED) ---\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, feature_dim, num_sensitive_groups):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Fatter Architecture\n",
    "            nn.Linear(feature_dim, 128), \n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(),\n",
    "            # Output size is now num_sensitive_groups (which is 2)\n",
    "            nn.Linear(64, num_sensitive_groups) \n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "# --- Container Model: Adversarial Pipeline (No change) ---\n",
    "class AdversarialPipeline(nn.Module):\n",
    "    def __init__(self, input_size, feature_dim, num_classes, num_sensitive_groups):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor(input_size, feature_dim)\n",
    "        self.classifier = Classifier(feature_dim, num_classes)\n",
    "        # num_sensitive_groups is 2 here\n",
    "        self.adversary = Adversary(feature_dim, num_sensitive_groups) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.feature_extractor(x)\n",
    "        y_pred = self.classifier(z)   \n",
    "        s_pred = self.adversary(z)    \n",
    "        \n",
    "        return y_pred, s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74cf8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pipeline.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim# Import models from Step 2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_validate(pipeline, train_loader, test_loader, device, epochs=10, feature_dim=32, lambda_adv=0.1):\n",
    "    \n",
    "    # 1. Optimizers Setup (Two separate ones)\n",
    "    optimizer_main = optim.Adam(\n",
    "        list(pipeline.feature_extractor.parameters()) + list(pipeline.classifier.parameters()),\n",
    "        lr=0.001\n",
    "    )\n",
    "    optimizer_adv = optim.Adam(pipeline.adversary.parameters(), lr=0.001)\n",
    "\n",
    "    # 2. Loss Functions\n",
    "    criterion_Y = nn.CrossEntropyLoss()\n",
    "    criterion_S = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pipeline.train()\n",
    "        total_y_loss, total_s_loss = 0, 0\n",
    "        \n",
    "        for x, y_true, s_true in train_loader:\n",
    "            x, y_true, s_true = x.to(device), y_true.to(device), s_true.to(device)\n",
    "            \n",
    "            # --- PHASE 1: Train the ADVERSARY (A) to be accurate ---\n",
    "            \n",
    "            # Ensure only A is updated\n",
    "            pipeline.feature_extractor.requires_grad_(False)\n",
    "            pipeline.classifier.requires_grad_(False)\n",
    "            pipeline.adversary.requires_grad_(True)\n",
    "            \n",
    "            optimizer_adv.zero_grad()\n",
    "            \n",
    "            # Forward pass: Crucial: .detach() z to prevent gradient flow to F/C\n",
    "            z_detached = pipeline.feature_extractor(x).detach()\n",
    "            s_pred_adv = pipeline.adversary(z_detached)\n",
    "            \n",
    "            loss_adv = criterion_S(s_pred_adv, s_true)\n",
    "            loss_adv.backward()\n",
    "            optimizer_adv.step()\n",
    "            total_s_loss += loss_adv.item()\n",
    "\n",
    "            # --- PHASE 2: Train F and C to be accurate on Y AND deceive A ---\n",
    "            \n",
    "            # Ensure F and C are updated, A is frozen\n",
    "            pipeline.feature_extractor.requires_grad_(True)\n",
    "            pipeline.classifier.requires_grad_(True)\n",
    "            pipeline.adversary.requires_grad_(False)\n",
    "            \n",
    "            optimizer_main.zero_grad()\n",
    "\n",
    "            # Full forward pass (no detach on Z)\n",
    "            y_pred, s_pred_deceive = pipeline(x)\n",
    "            \n",
    "            # Main objective: Minimize Y loss\n",
    "            loss_Y = criterion_Y(y_pred, y_true)\n",
    "            \n",
    "            # Adversarial objective: Maximize S loss (via negative sign)\n",
    "            loss_S_deceive = criterion_S(s_pred_deceive, s_true)\n",
    "            \n",
    "            # Total Main Loss: L_Y - lambda * L_S_deceive (Minimize this total)\n",
    "            total_main_loss = loss_Y - lambda_adv * loss_S_deceive\n",
    "            \n",
    "            total_main_loss.backward()\n",
    "            optimizer_main.step()\n",
    "            total_y_loss += loss_Y.item()\n",
    "\n",
    "        # --- Validation after each epoch ---\n",
    "        val_acc, val_s_acc = evaluate_model(pipeline, test_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | Avg Y Loss: {total_y_loss/len(train_loader):.4f} | Avg Adv Loss: {total_s_loss/len(train_loader):.4f}\")\n",
    "        print(f\"        | Val Y Acc: {val_acc*100:.2f}% | Val S Acc (Adversary success): {val_s_acc*100:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            # Save the best model based on primary task accuracy\n",
    "            save_checkpoint(pipeline, optimizer_main, optimizer_adv, epoch, best_val_accuracy)\n",
    "\n",
    "def evaluate_model(pipeline, data_loader, device):\n",
    "    pipeline.eval()\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    s_true_all, s_pred_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y_true, s_true in data_loader:\n",
    "            x = x.to(device)\n",
    "            y_pred_logits, s_pred_logits = pipeline(x)\n",
    "            \n",
    "            # Primary Task (Y) Accuracy\n",
    "            y_preds = torch.argmax(y_pred_logits, dim=1)\n",
    "            y_pred_all.extend(y_preds.cpu().numpy())\n",
    "            y_true_all.extend(y_true.cpu().numpy())\n",
    "            \n",
    "            # Adversary Task (S) Accuracy\n",
    "            s_preds = torch.argmax(s_pred_logits, dim=1)\n",
    "            s_pred_all.extend(s_preds.cpu().numpy())\n",
    "            s_true_all.extend(s_true.cpu().numpy())\n",
    "            \n",
    "    y_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "    s_acc = accuracy_score(s_true_all, s_pred_all)\n",
    "    return y_acc, s_acc\n",
    "\n",
    "def save_checkpoint(pipeline, optimizer_main, optimizer_adv, epoch, val_accuracy):\n",
    "    CHECKPOINT_PATH = \"best_adversarial_pipeline_checkpoint.pth\"\n",
    "    print(f\"--- Saving best model (Acc: {val_accuracy*100:.2f}%) ---\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': pipeline.state_dict(),\n",
    "        'optimizer_main_state_dict': optimizer_main.state_dict(),\n",
    "        'optimizer_adv_state_dict': optimizer_adv.state_dict(),\n",
    "        'val_accuracy': val_accuracy,\n",
    "    }\n",
    "    torch.save(checkpoint, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "804f40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pipeline.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Helper to freeze/unfreeze entire modules\n",
    "def toggle_grad(module, enable):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = enable\n",
    "\n",
    "        \n",
    "def train_and_validate(pipeline, train_loader, test_loader, device, epochs=10, feature_dim=32, lambda_adv=0.1, s_loss_weights=None):\n",
    "    \n",
    "    # 1. Optimizers Setup (Two separate ones)\n",
    "    optimizer_main = optim.Adam(\n",
    "        list(pipeline.feature_extractor.parameters()) + list(pipeline.classifier.parameters()),\n",
    "        lr=0.001\n",
    "    )\n",
    "    optimizer_adv = optim.Adam(pipeline.adversary.parameters(), lr=0.001)\n",
    "\n",
    "    # 2. Loss Functions\n",
    "    criterion_Y = nn.CrossEntropyLoss()\n",
    "    criterion_S = nn.CrossEntropyLoss(weight=s_loss_weights) # <-- USE WEIGHTS HERE\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pipeline.train()\n",
    "        total_y_loss, total_s_loss = 0, 0\n",
    "        \n",
    "        for x, y_true, s_true in train_loader:\n",
    "            x, y_true, s_true = x.to(device), y_true.to(device), s_true.to(device)\n",
    "            \n",
    "            # =====================================================================\n",
    "            # --- PHASE 1: Train the ADVERSARY (A) to be accurate ---\n",
    "            # Objective: Minimize Loss_S\n",
    "            # =====================================================================\n",
    "            \n",
    "            # 1. Ensure only Adversary parameters are active\n",
    "            toggle_grad(pipeline.feature_extractor, False)\n",
    "            toggle_grad(pipeline.classifier, False)\n",
    "            toggle_grad(pipeline.adversary, True) \n",
    "            \n",
    "            optimizer_adv.zero_grad()\n",
    "            \n",
    "            # Forward pass for A's loss: F must be run inside torch.no_grad() \n",
    "            # to ensure Z has no history, guaranteeing that F is not updated.\n",
    "            with torch.no_grad():\n",
    "                z = pipeline.feature_extractor(x)\n",
    "            \n",
    "            # Adversary uses the frozen feature Z\n",
    "            s_pred_adv = pipeline.adversary(z) \n",
    "            \n",
    "            # Calculate loss for A\n",
    "            loss_adv = criterion_S(s_pred_adv, s_true)\n",
    "            loss_adv.backward()\n",
    "            optimizer_adv.step()\n",
    "            \n",
    "            total_s_loss += loss_adv.item() # <-- NOW THIS SHOULD BE NON-ZERO\n",
    "\n",
    "            # =====================================================================\n",
    "            # --- PHASE 2: Train F and C to deceive A ---\n",
    "            # Objective: Minimize Loss_Y - Lambda * Loss_S\n",
    "            # =====================================================================\n",
    "            \n",
    "            # 2. Ensure F and C are active, A is frozen\n",
    "            toggle_grad(pipeline.feature_extractor, True)\n",
    "            toggle_grad(pipeline.classifier, True)\n",
    "            toggle_grad(pipeline.adversary, False)\n",
    "                \n",
    "            optimizer_main.zero_grad()\n",
    "\n",
    "            # Full forward pass for main loss calculation (Z history is tracked now)\n",
    "            y_pred, s_pred_deceive = pipeline(x) \n",
    "            \n",
    "            loss_Y = criterion_Y(y_pred, y_true)\n",
    "            loss_S_deceive = criterion_S(s_pred_deceive, s_true)\n",
    "            \n",
    "            total_main_loss = loss_Y - LAMBDA_ADV * loss_S_deceive \n",
    "            \n",
    "            total_main_loss.backward()\n",
    "            optimizer_main.step()\n",
    "            total_y_loss += loss_Y.item()\n",
    "\n",
    "        # --- Validation after each epoch ---\n",
    "        val_acc, val_s_acc = evaluate_model(pipeline, test_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | Avg Y Loss: {total_y_loss/len(train_loader):.4f} | Avg Adv Loss: {total_s_loss/len(train_loader):.4f}\")\n",
    "        print(f\"        | Val Y Acc: {val_acc*100:.2f}% | Val S Acc (Adversary success): {val_s_acc*100:.4f}%\")\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            # Save the best model based on primary task accuracy\n",
    "            save_checkpoint(pipeline, optimizer_main, optimizer_adv, epoch, best_val_accuracy)\n",
    "\n",
    "def evaluate_model(pipeline, data_loader, device):\n",
    "    pipeline.eval()\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    s_true_all, s_pred_all = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y_true, s_true in data_loader:\n",
    "            x = x.to(device)\n",
    "            y_pred_logits, s_pred_logits = pipeline(x)\n",
    "            \n",
    "            # Primary Task (Y) Accuracy\n",
    "            y_preds = torch.argmax(y_pred_logits, dim=1)\n",
    "            y_pred_all.extend(y_preds.cpu().numpy())\n",
    "            y_true_all.extend(y_true.cpu().numpy())\n",
    "            \n",
    "            # Adversary Task (S) Accuracy\n",
    "            s_preds = torch.argmax(s_pred_logits, dim=1)\n",
    "            s_pred_all.extend(s_preds.cpu().numpy())\n",
    "            s_true_all.extend(s_true.cpu().numpy())\n",
    "            \n",
    "    y_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "    s_acc = accuracy_score(s_true_all, s_pred_all)\n",
    "    return y_acc, s_acc\n",
    "\n",
    "def save_checkpoint(pipeline, optimizer_main, optimizer_adv, epoch, val_accuracy):\n",
    "    CHECKPOINT_PATH = \"best_adversarial_pipeline_checkpoint.pth\"\n",
    "    print(f\"--- Saving best model (Acc: {val_accuracy*100:.2f}%) ---\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': pipeline.state_dict(),\n",
    "        'optimizer_main_state_dict': optimizer_main.state_dict(),\n",
    "        'optimizer_adv_state_dict': optimizer_adv.state_dict(),\n",
    "        'val_accuracy': val_accuracy,\n",
    "    }\n",
    "    torch.save(checkpoint, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e087396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Initialized on mps ---\n",
      "Total Parameters: 15,524\n",
      "Starting Training for 25 epochs with Lambda=10\n",
      "Epoch 01 | Avg Y Loss: 0.4286 | Avg Adv Loss: 0.6968\n",
      "        | Val Y Acc: 91.59% | Val S Acc (Adversary success): 49.9874%\n",
      "--- Saving best model (Acc: 91.59%) ---\n",
      "Epoch 02 | Avg Y Loss: 0.1982 | Avg Adv Loss: 0.6941\n",
      "        | Val Y Acc: 94.12% | Val S Acc (Adversary success): 48.8822%\n",
      "--- Saving best model (Acc: 94.12%) ---\n",
      "Epoch 03 | Avg Y Loss: 0.1412 | Avg Adv Loss: 0.6934\n",
      "        | Val Y Acc: 95.58% | Val S Acc (Adversary success): 63.4514%\n",
      "--- Saving best model (Acc: 95.58%) ---\n",
      "Epoch 04 | Avg Y Loss: 0.1250 | Avg Adv Loss: 0.6941\n",
      "        | Val Y Acc: 95.83% | Val S Acc (Adversary success): 60.7887%\n",
      "--- Saving best model (Acc: 95.83%) ---\n",
      "Epoch 05 | Avg Y Loss: 0.1193 | Avg Adv Loss: 0.6930\n",
      "        | Val Y Acc: 95.86% | Val S Acc (Adversary success): 45.9935%\n",
      "--- Saving best model (Acc: 95.86%) ---\n",
      "Epoch 06 | Avg Y Loss: 0.1171 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 95.96% | Val S Acc (Adversary success): 46.8224%\n",
      "--- Saving best model (Acc: 95.96%) ---\n",
      "Epoch 07 | Avg Y Loss: 0.1150 | Avg Adv Loss: 0.6933\n",
      "        | Val Y Acc: 96.06% | Val S Acc (Adversary success): 51.9216%\n",
      "--- Saving best model (Acc: 96.06%) ---\n",
      "Epoch 08 | Avg Y Loss: 0.1134 | Avg Adv Loss: 0.6934\n",
      "        | Val Y Acc: 96.01% | Val S Acc (Adversary success): 49.0831%\n",
      "Epoch 09 | Avg Y Loss: 0.1117 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.08% | Val S Acc (Adversary success): 57.9251%\n",
      "--- Saving best model (Acc: 96.08%) ---\n",
      "Epoch 10 | Avg Y Loss: 0.1103 | Avg Adv Loss: 0.6933\n",
      "        | Val Y Acc: 96.06% | Val S Acc (Adversary success): 60.5878%\n",
      "Epoch 11 | Avg Y Loss: 0.1101 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.21% | Val S Acc (Adversary success): 40.9194%\n",
      "--- Saving best model (Acc: 96.21%) ---\n",
      "Epoch 12 | Avg Y Loss: 0.1087 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.23% | Val S Acc (Adversary success): 46.1693%\n",
      "--- Saving best model (Acc: 96.23%) ---\n",
      "Epoch 13 | Avg Y Loss: 0.1079 | Avg Adv Loss: 0.6934\n",
      "        | Val Y Acc: 96.33% | Val S Acc (Adversary success): 42.7280%\n",
      "--- Saving best model (Acc: 96.33%) ---\n",
      "Epoch 14 | Avg Y Loss: 0.1072 | Avg Adv Loss: 0.6933\n",
      "        | Val Y Acc: 96.33% | Val S Acc (Adversary success): 45.4911%\n",
      "Epoch 15 | Avg Y Loss: 0.1061 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.26% | Val S Acc (Adversary success): 60.7887%\n",
      "Epoch 16 | Avg Y Loss: 0.1057 | Avg Adv Loss: 0.6933\n",
      "        | Val Y Acc: 96.33% | Val S Acc (Adversary success): 49.6107%\n",
      "Epoch 17 | Avg Y Loss: 0.1047 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.43% | Val S Acc (Adversary success): 49.0329%\n",
      "--- Saving best model (Acc: 96.43%) ---\n",
      "Epoch 18 | Avg Y Loss: 0.1036 | Avg Adv Loss: 0.6933\n",
      "        | Val Y Acc: 96.41% | Val S Acc (Adversary success): 46.7219%\n",
      "Epoch 19 | Avg Y Loss: 0.1035 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.21% | Val S Acc (Adversary success): 56.9957%\n",
      "Epoch 20 | Avg Y Loss: 0.1035 | Avg Adv Loss: 0.6931\n",
      "        | Val Y Acc: 96.46% | Val S Acc (Adversary success): 52.1979%\n",
      "--- Saving best model (Acc: 96.46%) ---\n",
      "Epoch 21 | Avg Y Loss: 0.1018 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.66% | Val S Acc (Adversary success): 43.4313%\n",
      "--- Saving best model (Acc: 96.66%) ---\n",
      "Epoch 22 | Avg Y Loss: 0.1008 | Avg Adv Loss: 0.6930\n",
      "        | Val Y Acc: 96.63% | Val S Acc (Adversary success): 50.6405%\n",
      "Epoch 23 | Avg Y Loss: 0.1001 | Avg Adv Loss: 0.6932\n",
      "        | Val Y Acc: 96.61% | Val S Acc (Adversary success): 46.2195%\n",
      "Epoch 24 | Avg Y Loss: 0.0994 | Avg Adv Loss: 0.6930\n",
      "        | Val Y Acc: 96.61% | Val S Acc (Adversary success): 47.1238%\n",
      "Epoch 25 | Avg Y Loss: 0.0979 | Avg Adv Loss: 0.6929\n",
      "        | Val Y Acc: 96.63% | Val S Acc (Adversary success): 42.7531%\n",
      "\n",
      "--- Training Finished ---\n",
      "Best model checkpoint saved to 'best_adversarial_pipeline_checkpoint.pth'\n"
     ]
    }
   ],
   "source": [
    "# main_script.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "FEATURE_DIM = 32 # Size of the shared feature vector Z\n",
    "EPOCHS = 25\n",
    "LAMBDA_ADV = 10 # Hyperparameter: controls the strength of the debiasing pressure (0.1 to 1.0)\n",
    "\n",
    "# --- Initialize Model ---\n",
    "pipeline = AdversarialPipeline(\n",
    "    input_size=INPUT_SIZE, \n",
    "    feature_dim=FEATURE_DIM, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    num_sensitive_groups=NUM_SENSITIVE_GROUPS\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"--- Model Initialized on {DEVICE} ---\")\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in pipeline.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Starting Training for {EPOCHS} epochs with Lambda={LAMBDA_ADV}\")\n",
    "\n",
    "# --- Start Training ---\n",
    "train_and_validate(\n",
    "    pipeline, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    DEVICE, \n",
    "    EPOCHS, \n",
    "    FEATURE_DIM, \n",
    "    LAMBDA_ADV,\n",
    "    S_LOSS_WEIGHTS.to(DEVICE) # <-- PASS WEIGHTS TO TRAIN FUNCTION\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")\n",
    "print(\"Best model checkpoint saved to 'best_adversarial_pipeline_checkpoint.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csgy6643_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
